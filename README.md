# ML Engineer Technical Test



## ğŸ“ Architecture Overview

The system consists of three services:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     HTTP      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     MCP       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Web App      â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚     Agent       â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚   MCP Server    â”‚
â”‚   (Streamlit)   â”‚               â”‚    (FastAPI)    â”‚               â”‚   (FastMCP)     â”‚
â”‚   Port 8501     â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚   Port 8001     â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚   Port 8002     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       UI                           LangGraph Agent                   Data Tools
```

### 1. Web App (`/webapp`)
- Streamlit frontend
- Account selection dropdown
- Query input field
- Supports both streaming and non-streaming responses

### 2. Agent (`/agent`)
- FastAPI backend exposing the agent
- LangGraph-based agent with two nodes:
  - **Supervisor**: Deterministic orchestrator that fetches data via MCP tools
  - **Final Answer**: GPT-4o-mini generates the response based on retrieved context
- Uses a **fixed plan** to fetch all transcripts and emails

### 3. MCP Server (`/mcp_server`)
- FastMCP server using `streamable_http` transport
- Provides two tools:
  - `transcripts`: Retrieves call transcripts for an account
  - `emails`: Retrieves emails for an account

---


## ğŸš€ Quick Start

### Prerequisites

- Python 3.12+
- [uv](https://docs.astral.sh/uv/)
- uv pip install -e ".[agent,mcp_server,webapp]"

### 1. Set Environment Variables

```bash
# For OpenAI (default)
export OPENAI_API_KEY="your-openai-api-key"

# For Google Gemini (alternative)
export GOOGLE_API_KEY="your-google-api-key"
export LLM_PROVIDER="google"  # Set to "google" to use Gemini, defaults to "openai"
```

### 2. Run All Services

Open **three terminals**:

**Terminal 1 - MCP Server:**
```bash
python src/mcp_server/server.py
# Running on http://localhost:8002
```

**Terminal 2 - Agent API:**
```bash
python src/agent/api.py
# Running on http://localhost:8001
```

**Terminal 3 - Web App:**
```bash
streamlit run src/webapp/app.py
# Opens http://localhost:8501
```

### 3. Test the System

1. Open http://localhost:8501 in your browser
2. Select an account from the dropdown
3. Enter a question (e.g., "What are the main pain points discussed?")
4. Click "Ask Agent"


## Docker Setup (Optional)

Alternatively, you can run the entire system using Docker Compose.

1. Build and start the services:

```bash
docker-compose up --build
```
2. Access the web app at http://localhost:8003

---

## ğŸ“Š Dataset Format

Account data files are JSON with this structure:

```json
{
  "tenant_name": "string",
  "account_name": "string",
  "account_id": 123,
  "calls": [
    {
      "date": "2024-01-15",
      "call_name": "Discovery Call",
      "transcript": "...",
      "summary": "...",
      "crm_fields": [{"key": "value"}]
    }
  ],
  "emails": [
    {
      "date": "2024-01-16",
      "subject": "Follow-up",
      "content": "..."
    }
  ]
}
```

---

## ğŸ“ Project Structure

```
ml-eng-hiring/
.
â”œâ”€â”€ pyproject.toml              # Project metadata & dependencies
â”œâ”€â”€ docker-compose.yml          # Multi-service orchestration
â”œâ”€â”€ docker/                     # Dockerfiles for each service
â”‚   â”œâ”€â”€ agent.Dockerfile
â”‚   â”œâ”€â”€ mcp_server.Dockerfile
â”‚   â””â”€â”€ webapp.Dockerfile
â”œâ”€â”€ scripts/                    # Evaluation & automation scripts
â”‚   â”œâ”€â”€ aggregate_metrics.py
â”‚   â”œâ”€â”€ fill_topics.py
â”‚   â”œâ”€â”€ llm_as_judge.py
â”‚   â””â”€â”€ run_agent.py
â””â”€â”€ src/
    â”œâ”€â”€ agent/                  # LLM agent implementation
    â”‚   â”œâ”€â”€ api.py               # FastAPI server
    â”‚   â”œâ”€â”€ main.py              # Agent entry point
    â”‚   â”œâ”€â”€ graph.py             # LangGraph definition
    â”‚   â”œâ”€â”€ config.py            # Configuration & state
    â”‚   â”œâ”€â”€ llm_utils.py         # LLM helpers
    â”‚   â”œâ”€â”€ agent_graph.png      # Agent graph visualization
    â”‚   â”œâ”€â”€ nodes/               # Agent graph nodes
    â”‚   â”‚   â”œâ”€â”€ planner.py
    â”‚   â”‚   â”œâ”€â”€ plan_executer.py
    â”‚   â”‚   â”œâ”€â”€ mcp.py
    â”‚   â”‚   â””â”€â”€ final_answer.py
    â”‚   â””â”€â”€ README.md
    â”œâ”€â”€ mcp_server/              # MCP data server
    â”‚   â”œâ”€â”€ server.py            # MCP server implementation
    â”‚   â”œâ”€â”€ data/
    â”‚   â”‚   â”œâ”€â”€ accounts/        # Account JSON files
    â”‚   â”‚   â””â”€â”€ topics.json
    â”‚   â””â”€â”€ README.md
    â””â”€â”€ webapp/                  # Streamlit UI
        â”œâ”€â”€ app.py               # Web application
        â””â”€â”€ README.md
```
