import json
from typing import Any

from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel

from agent.llm_utils import get_llm
from mcp_server.server import load_account_data


class Evaluation(BaseModel):
    """Model for structured output containing evaluation scores."""

    answer_a_score: int
    answer_b_score: int


llm = get_llm(
    llm_provider="google", model_name="gemini-2.5-flash", reasoning_effort="none"
).with_structured_output(Evaluation)


JUDGE_SYSTEM_PROMPT = """You are an impartial judge tasked with evaluating the quality of responses generated by AI systems.
You will be provided with a user question, a context containing relevant information, and two different AI-generated answers (Answer A and Answer B).
Your job is to assess the relevance, accuracy, and completeness of each answer based on the provided context and user question and give two scores from 1 to 10 (10 being the best) for each answer.
You MUST provide your evaluation in the following JSON format:
{{
  "answer_a_score": <score for Answer A>,
  "answer_b_score": <score for Answer B>
}}"""


def evaluate_answer(
    user_question: str, context: str, answer_a: str, answer_b: str
) -> dict[str, int]:
    """Evaluate two AI-generated answers and return their scores."""
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", JUDGE_SYSTEM_PROMPT),
            (
                "human",
                "Context: {context}\nUser Question: {user_question}\nAnswer A: {answer_a}\nAnswer B: {answer_b}",
            ),
        ]
    )
    chain = prompt | llm
    response = chain.invoke(
        {
            "context": context,
            "user_question": user_question,
            "answer_a": answer_a,
            "answer_b": answer_b,
        }
    )
    return response  # type: ignore[no-any-return]


def concatenate_context(account_data: dict[str, Any]) -> str:
    """Concatenate account data into a single context string."""
    context_list = []
    for call in account_data["calls"]:
        context_list.append(f"Call on {call['date']}: {call['transcript']}")
    for email in account_data["emails"]:
        context_list.append(f"Email on {email['date']}: {email['content']}")
    return "\n".join(context_list)


def main(
    agent_results: dict[str, dict[int, str]],
    baseline_results: dict[str, dict[int, str]],
    output_file: str,
) -> None:
    """Main function to evaluate agent and baseline results."""
    evaluations = {"baseline": 0, "agent": 0}
    for account_id, agent_account_result in agent_results.items():
        if account_id not in baseline_results:
            continue
        baseline_account_result = baseline_results.get(account_id)
        account_data = load_account_data(int(account_id))
        context = concatenate_context(account_data)  # type: ignore[arg-type]
        user_question = agent_account_result["question"]  # type: ignore[index]
        answer_a = agent_account_result["response"]  # type: ignore[index]
        answer_b = baseline_account_result["response"]  # type: ignore[index]
        evaluation = evaluate_answer(user_question, context, answer_a, answer_b)
        evaluations["baseline"] += evaluation.answer_a_score  # type: ignore[attr-defined]
        evaluations["agent"] += evaluation.answer_b_score  # type: ignore[attr-defined]
    with open(output_file, "w") as f:
        json.dump(evaluations, f)


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Evaluate AI-generated answers.")
    parser.add_argument(
        "--agent-results",
        type=str,
        required=True,
        help="Path to agent results JSON file.",
    )
    parser.add_argument(
        "--baseline-results",
        type=str,
        required=True,
        help="Path to baseline results JSON file.",
    )
    parser.add_argument(
        "--output", type=str, required=True, help="Path to output evaluation JSON file."
    )
    args = parser.parse_args()
    with open(args.agent_results) as f:
        agent_results = json.load(f)
    with open(args.baseline_results) as f:
        baseline_results = json.load(f)
    main(agent_results, baseline_results, args.output)
